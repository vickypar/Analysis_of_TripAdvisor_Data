{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278a2519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vasil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vasil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import Series, DataFrame\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import collections\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "rcParams['figure.figsize'] = 20,12\n",
    "ps = PorterStemmer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27ec3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "reviews = pd.read_csv(r\"reviews_all_preprocessed.csv\")\n",
    "stop1 = ['food','good','nice','price','place','servic','restaur','visit','friendli','thessaloniki','order','great','dish','recommend','amaz','one','realli','us','serv','staff','qualiti','’','time', 'tri', 'experi', 'come', 'small', 'local', 'even', 'excel', 'reason', 'atmospher', 'best', 'love', 'enjoy', 'would', 'greek', 'well', 'also', 'fresh', 'back', 'like', 'tast', 'wine', 'definit', 'tabl', 'offer', 'menu', 'salad', 'drink', 'waiter', 'tradit', 'tasti', 'delici', 'day', 'peopl', 'go', 'meal', 'fish', 'dinner', 'eat', 'cook', 'portion', 'look', 'meat', 'citi', 'everyth', 'music', 'locat', 'lunch','town', 'warm', 'year', 'outsid', 'street', 'chicken', 'coffe', 'night', 'perfect', 'much', 'want', 'view', 'got', 'plate', 'littl', 'sea', 'expect', 'owner', 'ok', 'make', 'way', 'next', 'sit', 'wait', 'pizza', 'friend', 'cuisin', 'high', 'lot', 'differ', 'kind', 'noth', 'special', 'choic', 'decor', 'live', 'greec', 'alway', 'center', 'better', 'bread', 'ask', 'help', 'dont', 'get', 'mani', 'top', 'fantast', 'thing', 'say', 'two', 'first', 'didnt', 'busi', 'went', 'came', 'cours', 'though', 'polit', 'find', 'fast', 'end', 'grill', 'take', 'found', 'ever', 'quit', 'euro', 'wonder', 'everi', 'starter', 'bar', 'area', 'main', '2', 'full', 'hous', 'highli', 'could', 'burger', 'chees', 'made', 'big', 'valu', 'must', 'select', 'smoke', 'beauti', 'last', 'around', 'someth', 'varieti', 'money', 'feel', 'fri', 'beer', 'seafood', 'bit', 'insid', 'sure', 'worth', 'expens', 'disappoint', 'need', 'ate', 'famili', 'dessert','\\\"','”','“']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d94c1",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e68673bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataframe that contains the reviews and remove the extra stopwords\n",
    "reviews_lda = reviews.copy()\n",
    "#reviews_lda['Review'] = reviews_lda['Review'].apply(lambda x: [item for item in x if item not in stop1])\n",
    "\n",
    "#Convert \"Rating_date\" into datetime and sort reviews based on that\n",
    "reviews_lda['Rating_date'] = pd.to_datetime(reviews_lda['Rating_date'])\n",
    "reviews_lda.sort_values(by = 'Rating_date', inplace = True)\n",
    "\n",
    "#Keep only the columns that we need for LDA (Rating_date and Review)\n",
    "sorted_reviews = reviews_lda.drop(reviews.columns.difference(['Rating_date', 'Review']), 1)\n",
    "sorted_reviews.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#Convert each review into a list of the tokens it includes\n",
    "temp_uni_sorted = []\n",
    "for i in range(len(sorted_reviews)):\n",
    "    temp_uni_sorted.append(sorted_reviews['Review'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18b05744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create chronologically ordered groups, where each one contains 2000 reviews\n",
    "n = 2000\n",
    "split_accross_years = [temp_uni_sorted[i:i + n] for i in range(0, len(temp_uni_sorted), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f54ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "import gensim.corpora as corpora    \n",
    "from pprint import pprint\n",
    "\n",
    "# LDA for each group of reviews in chronological order\n",
    "for i in split_accross_years:\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(i)\n",
    "    # Create Corpus\n",
    "    texts = i\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "\n",
    "    # Define the number of topics\n",
    "    num_topics = 4    \n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                           id2word = id2word,\n",
    "                                           num_topics = num_topics)\n",
    "    \n",
    "    #Print the topics and the most important keywords\n",
    "    #pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    \n",
    "    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()] \n",
    "    \n",
    "    cloud = WordCloud(background_color = 'white',\n",
    "                      width = 2500,\n",
    "                      height = 1800,\n",
    "                      max_words = 15,\n",
    "                      colormap = 'tab10',\n",
    "                      color_func = lambda *args, **kwargs: cols[i],\n",
    "                      prefer_horizontal = 1.0)\n",
    "    \n",
    "    topics = lda_model.show_topics(formatted = False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize = (8,8), sharex = True, sharey = True)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        fig.add_subplot(ax)\n",
    "        topic_words = dict(topics[i][1])\n",
    "        cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "    \n",
    "    \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624605c",
   "metadata": {},
   "source": [
    "## Fastest Growing and Fastest Shrinking Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b15dbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Rating_date'] =pd.to_datetime(reviews_lda['Rating_date'])\n",
    "#we sort the dataframe by time so each chuck represents a time splot \n",
    "reviews.sort_values(by='Rating_date',inplace=True)\n",
    "test = reviews.copy()\n",
    "#n represents the size of each chunk we want to cut the dataframe\n",
    "n = 1597\n",
    "#x represents the array that consists of dictionaries on each cell and its size is the amount of chunks we have\n",
    "x = []\n",
    "#the loop stops when the dataframe is empty\n",
    "while len(test)>0:\n",
    "    #we just take the first n rows and then we proceed removing those rows after we have processed them\n",
    "    temp = test.head(n)\n",
    "    test = test.iloc[n: , :]\n",
    "    temp_uni = []\n",
    "    for i in range(len(temp)):\n",
    "        temp_uni += temp['Review'].iloc[i]\n",
    "    temp_uni = pd.Series(temp_uni)\n",
    "#we create a dictionary for each chuck that has the shape of word:frequency\n",
    "    x.append(temp_uni.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e705f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_uni = []\n",
    "for i in range(len(reviews)):\n",
    "    temp_uni += reviews['Review'].iloc[i]\n",
    "uni_words = {}\n",
    "\n",
    "#we find all the words and create a dictionary with them\n",
    "for i in temp_uni:\n",
    "    uni_words.update({i:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "232bea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word dictionary represents the for each worth the growth of it\n",
    "word = {}\n",
    "#dont_count dictionary exists so we can not count the words that dont exist in one or more time slots.\n",
    "dont_count = {}\n",
    "for i in range(len(x)-1):\n",
    "    for z in uni_words.keys():\n",
    "        try:\n",
    "            next_time = x[i+1][z]\n",
    "            try:\n",
    "                current_time = x[i][z]\n",
    "#here we calculate the growth of the specific time slot\n",
    "                temp = (next_time-current_time)/current_time\n",
    "            except:\n",
    "                temp = 1\n",
    "                dont_count.update({z:1})\n",
    "        except:\n",
    "            temp = -1\n",
    "            #dont_count.update({x[i+1][z]:1})\n",
    "            dont_count.update({z:1})\n",
    "        try:\n",
    "            word.update({z:temp+word[z]})\n",
    "        except:\n",
    "            word.update({z:temp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4f93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we transofrm the word dictionary in a dataframe and then we drop each word that exists in the dont_count dictionary\n",
    "Growth = pd.DataFrame.from_dict([word]).T\n",
    "for i in dont_count.keys():\n",
    "    Growth.drop(str(i),inplace = True)\n",
    "\n",
    "#here we get the top 10 and bottom 10 words that represent the positive and negative growth repsectively\n",
    "Top = Growth.sort_values(by = 0,ascending =False).head(12)\n",
    "Bottom = Growth.sort_values(by = 0,ascending =False).tail(10)\n",
    "stop2 = ['‘','nowher']\n",
    "Top.reset_index(inplace=True)\n",
    "Top = Top[Top[\"index\"].str.contains('‘')==False]\n",
    "Top = Top[Top[\"index\"].str.contains('nowher')==False]\n",
    "Top.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee794502",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "for i in range(len(x)):\n",
    "        word.append(x[i]['aristotel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fe7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f3e268",
   "metadata": {},
   "source": [
    "# Map of Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfe19b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6aec5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get county from city name\n",
    "def get_country_from_city_name(x):\n",
    "    try:\n",
    "        geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "        location = geolocator.geocode(x)\n",
    "        address = location.raw\n",
    "        return address.get('display_name').split(',')[-1]\n",
    "    except:\n",
    "        return 'Unknown Country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67a5a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get country from the city name\n",
    "def get_countries(x):\n",
    "    try:\n",
    "        array = x.split('from')[1]\n",
    "    except:\n",
    "        return 'no country given'\n",
    "    try:\n",
    "        return get_country_from_city_name(array.split(',')[1])\n",
    "    except:\n",
    "        try:\n",
    "            return get_country_from_city_name(array)\n",
    "        except:\n",
    "            return 'Miss_spelled_City_Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b051c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to get longitude and latitude data from country name\n",
    "def geolocate_latitude(country):\n",
    "        geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "        loc = geolocator.geocode(country)\n",
    "        return (loc.latitude)\n",
    "    \n",
    "def geolocate_longitude(country):\n",
    "        geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "        loc = geolocator.geocode(country)\n",
    "        return (loc.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6329203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = reviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['Country'] = testdf['Review_Distribution'].apply(lambda x: get_countries(x))\n",
    "testdf['Country'] = testdf['Country'].str.strip()\n",
    "testdf['Country'] = testdf['Country'].apply(lambda x: get_country_from_city_name(x))\n",
    "# The above cell takes a while to run so we will present you how the dataframe will be transformed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab125f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_map = pd.read_csv(r\"beforegetcontinent.csv\")\n",
    "reviews_map.drop(columns = [\"Unnamed: 0\"], inplace = True)\n",
    "reviews_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a704a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = testdf[testdf['Country'] != 'Unknown Country']\n",
    "testdf.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca26755",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['Latitude'] = testdf['Country'].apply(lambda x: geolocate_latitude(x))\n",
    "testdf['Longitude'] = testdf['Country'].apply(lambda x: geolocate_longitude(x))\n",
    "# The above cell takes a while to run so we will present you how the dataframe will be transformed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff74303",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_map = pd.read_csv(r\"latlon.csv\")\n",
    "reviews_map.drop(columns = [\"Unnamed: 0\"], inplace = True)\n",
    "reviews_map.drop(columns = [\"Unnamed: 0.1\"], inplace = True)\n",
    "reviews_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = reviews_map.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a world map to show distributions of users \n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "#empty map\n",
    "world_map= folium.Map(tiles=\"cartodbpositron\")\n",
    "marker_cluster = MarkerCluster().add_to(world_map)\n",
    "for i in range(len(testdf)):\n",
    "        lat = testdf.iloc[i]['Latitude']\n",
    "        long = testdf.iloc[i]['Longitude']\n",
    "        radius=5\n",
    "        folium.CircleMarker(location = [lat, long], radius=radius, fill =True).add_to(marker_cluster)\n",
    "#show the map\n",
    "world_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7461d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
